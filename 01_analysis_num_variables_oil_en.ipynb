{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Demosthene-OR/Student-AI-and-Data-Management/blob/main/01_analysis_num_variables_oil_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "<img src=\"https://prof.totalenergies.com/wp-content/uploads/2024/09/TotalEnergies_TPA_picto_DegradeRouge_RVB-1024x1024.png\" height=\"150\" width=\"150\">\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<h1 style = \"text-align:center\" >Exploratory statistics</h1> \n",
    "<h2 style = \"text-align:center\">Descriptive analysis of the digital variables of a dataset</h2> \n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    "### Background and objective\n",
    "\n",
    "> In Data Science, before starting processing and modeling, it is very important to familiarize yourself with the dataset you have available. Analyzing variables is an essential step in data exploration.\n",
    ">\n",
    ">\n",
    "> The objective of this module is therefore to understand the basics of statistics and go a little further in their interpretation to perform exploratory statistics.\n",
    ">\n",
    ">\n",
    "> In this first notebook, we will focus on **numerical variables**.\n",
    ">\n",
    "> \n",
    "> **`pandas`** , thanks to the `DataFrame` class and its methods, allows us to quickly obtain descriptive statistics for quantitative variables. This will be the main tool used to conduct these studies, presented in the first part of this notebook.\n",
    ">\n",
    ">\n",
    "> In the second part of this notebook, we will introduce some concepts related to probability distributions, in particular the **normal distribution**. First, we will learn how to simulate data from a distribution using the **`numpy`** library.\n",
    ">\n",
    ">\n",
    ">  Then, to check whether a variable has a distribution similar to a normal distribution, we will use a **Quantile-Quantile plot** (*QQ-plot*) with a function from the **`statsmodels.api`** library.\n",
    ">\n",
    ">\n",
    "> To conclude this notebook, we will study the **correlation** between two numerical variables using the correlation coefficient.\n",
    ">\n",
    "> \n",
    "\n",
    "Let's start with the package import phase.\n",
    "\n",
    "* **(a)** Import the **`pandas`** and **`numpy`** packages under their usual aliases.\n",
    "\n",
    "\n",
    "* **(b)** Load the data contained in the **`‘OilWell_Production_Maintenance.csv’`** (wells daily measurement) file into a **`DataFrame`** named **`df`**.\n",
    "\n",
    "\n",
    "* **(c)** Display the first 5 rows of **`df`**.\n",
    "\n",
    "> Column explanations:  \n",
    "> * Date: measurement day.  \n",
    "> * Well_ID: well identifier (W1, W2…).  \n",
    "> * Flow_bbl_day: daily production in barrels.  \n",
    "> * Pressure_bar: pressure in the well (bar).  \n",
    "> * Temperature_C: temperature measured in the well.  \n",
    "> * Vibration_level: equipment vibration, indicator of wear (0 = normal, >0.05 = risk).  \n",
    "> * Maintenance_done: “Yes/No” if maintenance was performed that day.  \n",
    "> * Maintenance_duration_h: maintenance duration in hours.  \n",
    "> * Incident: “Yes/No” if an incident occurred that day (leak, failure, alarm).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Demosthene-OR/Student-AI-and-Data-Management/main/data/\"\n",
    "df = pd.read_csv(url+\"OilWell_Production_Maintenance.csv\")\n",
    "pd.to_datetime(df['Date'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<h3 style = \"text-align:center\" >1. Statistical series, position and dispersion indicators</h3> \n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    "> A **statistical series** is a list of values from the same set. The order of the terms is not significant (unlike a **time series**, where the elements depend on their position in time).<br>\n",
    "\n",
    "* **(d)** Display the unique values taken by the **`“Maintenance_duration_h”`** and **`“Maintenance_done”`** columns. To do this, we can use the **`unique`** method of `Series` `pandas`. These two results constitute two statistical series (quantitative and qualitative, respectively).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quantitative statistical series:\n",
    "print(\"Possible values for the 'maintenance duration'  column :\\n\", df[\"Maintenance_duration_h\"].unique())\n",
    "print(\"--------------------------------------------------------\")\n",
    "\n",
    "# Qualitative statistical series:\n",
    "print(\"Possible values for the 'maintenance done' column  :\\n\", df[\"Maintenance_done\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "> A quick way to distinguish a quantitative variable from a qualitative variable is to look at the number of unique values taken by that variable. If this number is high, we are most likely dealing with a quantitative variable. In what follows, we will focus only on quantitative variables.\n",
    ">\n",
    "> To better visualize the **type** of each column in a `DataFrame` and the **number of non-missing values**, we use the **`info`** method of the `DataFrame` class, which displays a short summary of the **volume of data** contained in a `DataFrame`.\n",
    "\n",
    "* **(e)** Display a summary of the `DataFrame` **`df`** created previously. How many variables are non-numeric? What type are they?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()\n",
    "\n",
    "# Three variables are non-numeric. \n",
    "# They are character strings and therefore probably qualitative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "> The **`dtypes`** attribute allows us to retrieve the types of variables we have in the form of a `pandas` `Series`. Since this object is a `Series`, it allows us to calculate statistics on these types using methods such as **`value_counts`**.\n",
    "\n",
    "* **(f)** Use the **`dtypes`** attribute and the **`value_counts`** method to count the number of variables of each type present in the **`df`** `DataFrame`. This type of inspection is very common when working with large databases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "solution"
   },
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "> To perform a descriptive analysis of a numerical variable, we use:\n",
    ">\n",
    "> * **Position indicators** (mean, median, quantiles, minimum, maximum, etc.) that allow us to **locate** the values that the variable we are studying should take.\n",
    ">\n",
    ">\n",
    "> * **Dispersion indicators** (standard deviation, variance, etc.) that describe the **variability** of the values taken by the variable we are studying.\n",
    ">\n",
    "> To better understand position indicators, we will refer to the following diagram:\n",
    ">\n",
    "> <br><br>\n",
    ">\n",
    "> <img src=\"https://assets-datascientest.s3.eu-west-1.amazonaws.com/104_stats_explo/indicateurs_positions.png\" style=\"height:300px\"> \n",
    ">\n",
    "> <br><br>\n",
    ">\n",
    ">\n",
    "> The small squares represent the values whose distribution we are studying.\n",
    "> \n",
    "> A **quantile** of order $\\alpha$ corresponds to a numerical value that must be **greater than** a proportion $\\alpha$ of the data.\n",
    ">\n",
    "> For example:\n",
    "> * If the quantile of order 0.10 is 1000, this means that 10% of the data is less than 1000.\n",
    "> * Similarly, if a quantile of order 0.57 is 2500, this means that 57% of the data is less than 2500.\n",
    ">\n",
    ">\n",
    "> **QUARtiles** are three special quantiles that divide a distribution into **four** intervals, each containing 25% of the data. In the diagram above, we note the quantiles $Q_1$, $Q_2$ and $Q_3$ and call them the first, second and third quartiles, respectively. These quartiles have interesting properties:\n",
    ">\n",
    "> * 25% of the data is below $Q_1$.\n",
    ">\n",
    ">\n",
    "> * 50% of the data is below $Q_2$. $Q_2$ corresponds to the **median** of the distribution.\n",
    ">\n",
    ">\n",
    "> * 75% of the data is below $Q_3$.\n",
    ">\n",
    ">\n",
    "> * The interval from $Q_1$ to $Q_3$ contains 50% of the data. The range $(Q_3 - Q_1)$ of this interval is called the **interquartile range**. It is denoted by $IQR$ for *Inter Quartile Range*.\n",
    ">\n",
    ">\n",
    "> * Any value **greater than** $Q_3 + 1.5 * (Q_3 - Q_1)$ (third quartile + 1.5 times the interquartile range) or **less than** $Q_1 - 1.5 * (Q_3 - Q_1)$ (first quartile - 1.5 times the interquartile range). This means that an extreme value can be very large **or** very small compared to the rest of the data.\n",
    ">\n",
    "> **Please note** that it is very common for the terms “extreme value” and “outlier” to be used interchangeably, but there is a clear distinction between the two.\n",
    ">\n",
    "> * An outlier is a value **that should not exist** or **should not be part of the distribution**. For example, if we study the height of individuals in a population, an individual with a height of -10 cm is clearly an outlier. If, by mistake, the dataset contains measurements of the height of dogs or horses (for example), these values are also outliers because they should not be in the distribution we want to study, but these values will not necessarily be extreme.\n",
    ">\n",
    "> \n",
    "> * An extreme value is a value that is **significantly higher or lower than the other values, but is not necessarily an outlier**. In the same example, an adult could be 2.50 m or 1.10 m tall, but these values are possible and realistic.\n",
    ">\n",
    ">\n",
    "> In any case, **we will want to eliminate outliers from our dataset** so as not to skew our statistics. However, in some cases, we will want to keep extreme values because they are real values and removing them would skew our statistics.\n",
    ">\n",
    ">\n",
    "> In a business setting, in an ideal situation, it is the role of the team conducting the analysis to report extreme or outlier values, and it is the role of the team in charge of data collection to determine which values are outliers and which values are extreme (however, in practice, it is difficult to have such a clear distinction between roles).\n",
    ">\n",
    "> The methods of the `Series` class used to calculate these indicators are summarized in the following table:\n",
    ">\n",
    "> | Method | Indicator | Example |\n",
    "> |---:|--|:--|\n",
    "> | **`mean`** | Mean | **`df[‘column’].mean()`** |\n",
    "> | **`min`** | Minimum | **`df[‘column’].min()`** |\n",
    "> | **`max`** | Maximum | **`df[‘column’].max()`** |\n",
    "> | **`median`** | Median | **`df[‘column’].median()`** |\n",
    "> | **`quantile`** | Quantile | **`df[‘column’].quantile(q=0.75)`** (returns the third quartile) <br><br> **`df[‘column’]. quantile(q=[0.1, 0.9])`** (returns the 0.1 quantile **and** the 0.9 quantile) |\n",
    "\n",
    "* **(g)** For the column **`“Vibration_level”`**, find the **minimum**, **maximum**, **median**, and **quartiles** of this variable by applying the appropriate methods.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Column analysed\n",
    "col = 'Vibration_level'\n",
    "\n",
    "print(\"The minimum value is: \", df[col].min())\n",
    "print(\"The maximum value is: \", df[col].max())\n",
    "print(\"The median value is: \", df[col].median(), '\\n')\n",
    "\n",
    "q1, q2, q3 = df[col].quantile(q=[0.25, 0.5, 0.75])\n",
    "\n",
    "print(\"The quartiles are:\", \"q1 =\", q1, \", q2 =\", q2, \", q3 =\", q3, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "* **(h)** Calculate the range of the **interquartile range** and determine the **thresholds** above which a value will be considered **extreme**.\n",
    "\n",
    "\n",
    "* **(i)** Using these thresholds, filter the `DataFrame` **`df`** to identify all individuals whose cholesterol levels are extreme values. Which levels are outliers? (We assume that a human can have a level of up to approximately 600 mg/dL, but cannot have a level of zero).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extreme values:\n",
    "# Calculation of thresholds:\n",
    "iqr = q3-q1\n",
    "threshold_min = q1 - 1.5*iqr\n",
    "threshold_max = q3 + 1.5*iqr\n",
    "\n",
    "print(\"Extreme values are all values less than\", threshold_min,\n",
    "      \"and all values greater than\", threshold_max)\n",
    "\n",
    "# Conclusions:\n",
    "print(f\"\"\"\n",
    "• Measurements whose {col} levels are above the interquartile range \n",
    "have realistic values. \\033[1mThese are outliers. \\033[0m\"\"\")\n",
    "display(df[df[col] > threshold_max])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "• Measurements whose {col} levels are below the interquartile range \n",
    "\\033[1mThese are therefore outliers.\\033[0m\n",
    "\"\"\")\n",
    "display(df[df[col] < threshold_min])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "> The **mean** of a numerical statistical series $X = (x_1, x_2, ..., x_n)$ is given by the formula:\n",
    ">\n",
    ">\n",
    "> $$\\hat{X}= \\displaystyle \\frac{1}{n} \\sum_{i=1}^{n} x_i$$\n",
    ">\n",
    "> where:\n",
    "> * $n$ is the **size** of the sample\n",
    ">\n",
    "> * $x_i$ are the **values** of the sample.\n",
    ">\n",
    "> The mean of a series is therefore simply the sum of the sample values divided by the size of the sample.\n",
    "\n",
    "* **(j)** Calculate the mean of the **`\"Vibration_level\"`** column.\n",
    "\n",
    "\n",
    "* **(k)** Compare this mean to the median of the column. How could we explain this difference?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculation “by hand”:\n",
    "X = df[\"Vibration_level\"]\n",
    "n = len(X) # sample size\n",
    "mean_X = (1/n)*np.sum(X)\n",
    "print(\"Mean calculated 'by hand': \", mean_X)\n",
    "\n",
    "# Quick command in Python:\n",
    "mean_X2 = df[\"Vibration_level\"].mean()\n",
    "print(\"Mean calculated with Python command: \", mean_X2)\n",
    "print(\"\\n\")\n",
    "\n",
    "# median median versus:\n",
    "print(\"The median  is equal to\", q2, \"and is higher than the mean.\")\n",
    "\n",
    "print('''\n",
    "The mean is influenced by extreme values. \n",
    "It is lower than the median because there are many extremely low values \n",
    "(172 values equal to 0 versus 11 values greater than {}) that “pull” the mean down.\n",
    "'''.format(threshold_max))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "> **The median is much more robust to extreme values**, making it a much more **reliable** position indicator in practice. The same is true for quantiles in general. In practice, quantiles of order 0.05 and 0.95 are better indicators of the range of the distribution than the minimum or maximum.\n",
    ">\n",
    ">\n",
    "We will now attempt to visualize the distribution of a variable using a very specific type of graph: **boxplots**.\n",
    ">\n",
    ">\n",
    "> A boxplot seeks to **visually represent a distribution using position and dispersion indicators.**\n",
    ">\n",
    "> The indicators represented in a boxplot are as follows:\n",
    ">\n",
    "> * **Position**: The first quartile $Q_1$, the median or second quartile $Q_2$, and the third quartile $Q_3$.\n",
    ">\n",
    ">\n",
    "> * **Dispersion**: The interquartile range $IQR$.\n",
    ">\n",
    "> Here is an explanatory diagram of the boxplot: \n",
    ">\n",
    "> <img src = \"https://assets-datascientest.s3.eu-west-1.amazonaws.com/104_stats_explo/boxplot_explained.png\" style=\"height:300px\">\n",
    ">\n",
    "> The whiskers represent the range of values that are considered *normal*. **Beyond these whiskers**, the points represented are considered **extreme values**.\n",
    ">\n",
    "> Here is an example of a boxplot plotted with Python for the variable **`“cholesterol”`** from our dataset:\n",
    "> \n",
    "> <img src = \"https://assets-datascientest.s3.eu-west-1.amazonaws.com/104_stats_explo/boxplot_cholesterol.png\" style=\"height:200px\">\n",
    ">\n",
    "> There are several elements of interest when reading a boxplot:\n",
    ">\n",
    "> * **Is the interquartile range small?** If so, this means that the distribution of the variable is concentrated around the median. If not, the distribution is more dispersed.\n",
    ">\n",
    ">\n",
    "> * **Are there any extreme values?** If so, they will need to be inspected to determine whether they are extreme or outliers.\n",
    ">\n",
    "> Using the boxplot of the variable `“Vibration_level”`, we could have directly and visually determined that the lower extreme values are **outliers** (because they are all equal to 0), while the upper **extreme values** are simply exceptional individuals, because their values are realistic.\n",
    ">\n",
    "> With Python, the easiest way to plot a boxplot is to use the **`seaborn`** library and its **`boxplot`** function. In the rest of the training, you will learn more about using seaborn.\n",
    ">\n",
    "> ```py\n",
    "> # Import the seaborn library under the alias sns\n",
    "> import seaborn as sns\n",
    ">\n",
    "> # Plotting a boxplot from the values in the “Vibration_level” column\n",
    "> sns.boxplot(df[‘Vibration_level’])\n",
    "> ```\n",
    "\n",
    "* **(l)** Import the **`seaborn`** library under the alias **`sns`**. <br>\n",
    "Using the **`.boxplot()`** command, display the boxplot of the variable **`“Vibration_level”`** and add the mean to the boxplot.\n",
    "`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.boxplot(x = df[col],showmeans=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "> Like the mean, the **standard deviation** is an indicator that is not represented graphically in a boxplot. The standard deviation measures the dispersion of data in a sample. In the following image, we can see how the distribution changes when the standard deviation (denoted by the letter sigma: **$\\sigma$**) varies for values of $5,10$ and $20$.\n",
    "> <img src=\"https://assets-datascientest.s3.eu-west-1.amazonaws.com/104_stats_explo/sigmas_bon.png\" style=\"height:300px\"> \n",
    ">\n",
    "> The greater the standard deviation, the more dispersed the data and the flatter the curve. <br>\n",
    "> The blue curve ($\\sigma = 20$) is much flatter than the yellow curve ($\\sigma = 5)$.\n",
    ">\n",
    "> Mathematically, the **standard deviation** of a numerical statistical series $X = (x_1, x_2, ..., x_n)$ is calculated using the following formula: <br>\n",
    ">\n",
    "> $$\\hat \\sigma_{X}= \\sqrt{\\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} (x_i-\\hat X)^2}$$\n",
    "> where: <br>\n",
    "> * $n =$ the sample size\n",
    ">\n",
    ">* $x_i$ are the **values** of the series \n",
    ">\n",
    "> * $\\hat X =$ the mean.\n",
    "\n",
    "\n",
    "* **(m)** As before, calculate the standard deviation of the **`“Vibration_level”`** column “by hand” and then using the **`std`** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculation by hand: \n",
    "X = df[col]\n",
    "n = len(X) # sample size\n",
    "std_X = np.sqrt((1/n)*sum((X-mean_X)**2))\n",
    "print(\"Standard deviation calculated 'by hand': \", std_X )\n",
    "\n",
    "# Quick command in Python: \n",
    "std_X2 = df[col].std()\n",
    "print(\"Standard deviation calculated with the Python command: \", std_X2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "* **(n)** To obtain the values of all these indicators for each numeric column of the `Dataframe`, you can directly use the **`describe`** method of `pandas.DataFrame`.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "solution"
   },
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<h3 style = \"text-align:center\" >  2. Normal distribution and data simulation</h3>  \n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<i class=\"fa fa-info-circle\"></i> &emsp; \n",
    "Much of the data used every day is very similar to variations of common probability distributions or combinations of distributions. That is why it is always useful to know how to <strong>simulate</strong> data from a <strong>theoretical probability</strong> distribution (normal, exponential, etc.).</div>\n",
    "\n",
    "\n",
    "> The **normal distribution**, often denoted $\\mathcal{N}(\\mu, \\sigma^2)$, is a **continuous** probability distribution, meaning that it takes values in an infinite set. It depends on two parameters: $\\mu$ (the Greek letter *mu*, representing the theoretical mean) and $\\sigma$ (the Greek letter *sigma*, representing the theoretical standard deviation). \n",
    ">\n",
    ">The **standard normal distribution** is a special case of the normal distribution with $\\mu = 0$ (centered around 0) and $\\sigma = 1$ (standardized).\n",
    ">\n",
    "> Here is the distribution of a **centered theoretical normal distribution ($\\mu = 0$)**, denoted by $\\mathcal{N}(\\mu=0, \\sigma^2)$:\n",
    ">\n",
    ">\n",
    "> <img src=\"https://assets-datascientest.s3.eu-west-1.amazonaws.com/104_stats_explo/bell_curve.png\" style=\"height:200px\"> \n",
    ">\n",
    "> When looking at a probability density, to represent probabilities, one must always think in terms of **s\n",
    "> \n",
    "> When looking at a probability density, to visualize probabilities, you must always think in terms of the **area** under the density curve. The area represents the probabilities of events. The total area under the curve (here, the sum of all the blue fragments) is equal to $1$. <br>\n",
    ">\n",
    "> We also note that much of the data from this distribution is close to $0$ because the area under the curve is large around this value. Whereas for values greater than $+2\\sigma$ and less than $-2\\sigma$, which correspond to the areas in **light blue**, the area under the curve decreases more and more, so the probability of obtaining such values is lower.\n",
    ">\n",
    "<div class=\"alert alert-info\">\n",
    "<i class=\"fa fa-info-circle\"></i> &emsp;\n",
    "    To simulate a distribution in Python, use\n",
    "    <code>np.random.normal(mu = ..., sigma = ..., size = ...)</code>\n",
    "    and specify the values of the parameters specific to the distribution.\n",
    "</div>\n",
    "\n",
    "* **(o)** Simulate $100$ draws (to be specified in the **`size`** parameter) from a centered normal distribution (to be specified in the **`mu`** parameter) and reduced (to be specified in the **`sigma`** parameter).  \n",
    "\n",
    "\n",
    "> We can display a histogram from a sample using the **`.histplot()`** command from the **`seaborn`** library: \n",
    "> \n",
    "> ```py\n",
    "> sns.histplot(sample)\n",
    "> ```\n",
    "\n",
    "* **(p)** Using the **`.histplot()`** command from the **`seaborn`** library, display the histogram of this variable. If you re-execute the cell, you will get a different histogram because your data has been re-generated.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu, sigma = 0, 1\n",
    "sn_distribution = np.random.normal(mu, sigma, size = 100)\n",
    "sns.histplot(sn_distribution,bins=21);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<i class=\"fa fa-info-circle\"></i> &emsp; \n",
    "    Since we are working with <b>simulated data</b>, the simulated distribution differs from the theoretical distribution. However, both distributions follow the same trend, and the more data there is, the closer the simulated histogram gets to the theoretical distribution. You can try increasing the <strong> <code>size</code></strong> parameter, for example to $10000$, and observe the histogram.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> The simulated values change each time you run the cell because they are new draws from the same distribution. If you always want to keep the same sample, you can use the **`np.random.seed()`** function and choose a **`seed`** parameter equal to any integer.\n",
    "\n",
    "* **(q)** Repeat the same question, increasing the sample size to $10,000$ and specifying an integer for the value of **`seed`** before generating your data. Now your experiment is reproducible; if you re-run the cell, you will get the same distribution.\n",
    "\n",
    "Translated with DeepL.com (free version)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"We observe that the more data we have from a normal distribution,\" + \"\\n\" +\n",
    "    \"the closer the histogram approaches the probability distribution of a theoretical normal distribution.\")\n",
    "\n",
    "np.random.seed (15)\n",
    "sn_distribution = np.random.normal(mu, sigma, 10000)\n",
    "sns.histplot(sn_distribution, bins=21);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<h3 style = \"text-align:center\" > 3. Data normality</h3>  \n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    "> The **Q-Q plot**, or **quantile** graph, allows us to compare the **theoretical quantiles of a distribution (by default those of a normal distribution)** with the quantiles of the **sample provided**. \n",
    ">- If the sample data comes from a normal distribution, we expect this graph to be **close to a straight line (called the first bisector) that forms** a $45°$ angle with the x-axis. This is because the quantiles of the sample will be similar to the theoretical quantiles of a normal distribution. If the sample data comes from a different distribution, we will not obtain points aligned on the first bisector.\n",
    ">\n",
    "> <img src = \"https://assets-datascientest.s3.eu-west-1.amazonaws.com/104_stats_explo/qqplot.png\" style = \"height:1000px\">\n",
    ">\n",
    "> ```py\n",
    "> # Import the statsmodels.api library\n",
    "> import statsmodels.api as sm\n",
    ">\n",
    "> # Create the Q-Qplot \n",
    "> sm.qqplot(sample, fit = True, line = ‘45’)\n",
    "> ```\n",
    ">\n",
    ">- The argument **`line = '45'`** displays the first bisector in red.\n",
    ">\n",
    ">- The argument **`fit = True`** centers and reduces the sample data.\n",
    "> \n",
    "\n",
    "* **(r)** Generate a sample **`ech`** of $100$ data points from a normal distribution with $\\mu = 12$ and $\\sigma = 3$. <br>  \n",
    "\n",
    "\n",
    "* **(s)** Import **`statsmodels.api`** under the alias **`sm`** and apply the **`qqplot`** function from the **`statsmodels.api`** library to **`ech`**, specifying the parameter **`line = ‘45’`** and normalizing the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "ech = np.random.normal(12, 3, 100)\n",
    "sm.qqplot(ech, fit = True, line = '45');\n",
    "# Note that if we remove the parameter fit = True, we do not observe \n",
    "# any similarities between ech and a centered and reduced normal distribution,\n",
    "# which is normal because ech was generated by a normal distribution with a mean of 3 and a standard deviation of 3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "> This distribution approximates a normal distribution despite slight distortions at the tails.\n",
    "Furthermore, the alignment is not perfect despite the fact that the sample comes from independent draws from a normal distribution, because we are comparing theoretical quantiles with empirical quantiles, i.e., those derived from a sample.\n",
    "Finally, we note that if we increase the sample size to $1000$ or $10000$, we can obtain points that are much more aligned around the red line.\n",
    ">\n",
    "* **(t)** Using the **`select_dtypes`** method, select only the numeric columns (**`int`** or **`float`**) of the **`df`** in a new `Dataframe` **`var_num`**.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "solution"
   },
   "outputs": [],
   "source": [
    "var_num = df.select_dtypes(include = ['int', 'float'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "\n",
    "* **(u)** To identify columns in the `DataFrame` **`df`** that approximate a normal distribution, display a Q-Q plot for each numeric column using a loop and determine which columns appear to follow a normal distribution.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop to display the Q-Q plots: \n",
    "for column in var_num.columns:\n",
    "    sm.qqplot(var_num[column], line='45', fit = True)\n",
    "    plt.title(f\"Q-Q Plot for {column}\")\n",
    "\n",
    "# The variables age and max_card_freq appear to follow normal distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<h3 style = \"text-align:center\" > 4. Correlation between two numerical variables</h3>  \n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    "> The correlation between two numerical variables $X$ (e.g., a person's age) and $Y$ (e.g., their height) allows us to quantify the relationship between the values of these two variables.\n",
    "> The correlation, denoted by $r$, is given by the formula:\n",
    ">\n",
    "> $$\\hat r(X,Y) = \\dfrac{\\hat {\\mathrm{cov}}(X,Y)}{\\hat\\sigma_X \\times \\hat\\sigma_Y}$$\n",
    "> where:\n",
    ">\n",
    "> * $\\hat{\\mathrm{cov}}(X,Y)$ is the covariance between $X$ and $Y$\n",
    ">\n",
    "> * $\\hat \\sigma_X$ is the standard deviation of $X$\n",
    ">\n",
    "> * $\\hat \\sigma_Y$ is the standard deviation of $Y$.\n",
    ">\n",
    "> By definition, the correlation **always** takes values between $[-1,1]$. We refer to a correlation as:\n",
    "> * **strong** if $\\hat r \\in [-1, -0.5] \\text{ or } [0.5, 1]$ \n",
    "> * or **weak** if $\\hat r \\in [-0.5, 0] \\text{ or } [0, 0.5]$. <br>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<i class=\"fa fa-info-circle\"></i> &emsp;     \n",
    "It should be noted that these thresholds are for informational purposes only and that the interpretation of a correlation coefficient depends on the context and objectives. A correlation of 0.9 may be <strong>very low</strong> when testing quantities of <strong>chemical substances</strong> using high-quality instruments, but may be considered <strong>very high</strong> in the <strong>social sciences</strong>, where there may be a greater contribution from complicating factors.</div>\n",
    "<div class=\"alert alert-info\">\n",
    "<i class=\"fa fa-info-circle\"></i> &emsp; \n",
    "Correlation is a measure that is <strong>sensitive</strong> to extreme values.</div>\n",
    "\n",
    "\n",
    "> Graphically, three different configurations can be distinguished: <br>\n",
    "> <img src=\"https://assets-datascientest.s3.eu-west-1.amazonaws.com/104_stats_explo/corr_drawio.png\" style=\"height:200px\">   \n",
    ">\n",
    ">\n",
    "> With the **`numpy`** library, we can calculate the correlation between two variables.\n",
    "> ```py\n",
    "> # Calculate the correlation between X and Y\n",
    "> np.corrcoef(X, Y)\n",
    "> ```\n",
    "> The result returns a correlation matrix that corresponds to:\n",
    "$ \n",
    "\\begin{bmatrix} \n",
    "\\hat r(X,X) = 1 & \\hat r(X,Y) \\\\\n",
    "\\hat r(Y, X) & \\hat r(Y,Y) = 1\\\\\n",
    "\\end{bmatrix}\n",
    "$.\n",
    ">\n",
    "> Note that the diagonal is always made up of values equal to $1$ because we are calculating the correlation between a variable and itself.\n",
    "\n",
    "* **(v)** Calculate the correlation between the **\"Flow_bbl_day\"** and **\"Pressure_bar\"** columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "solution"
   },
   "outputs": [],
   "source": [
    "np.corrcoef(df[\"Flow_bbl_day\"], df[\"Pressure_bar\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "* **(w)** You can also use the **`corr`** method of the `DataFrame` class to display the correlations between all the different numerical variables. Display these correlations.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.corr(numeric_only=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<i class=\"fa fa-info-circle\"></i> &emsp; \n",
    "Finally, be careful not to confuse correlation with causation. <br>\n",
    "Correlation means that there is a statistical association between variables. Causality means that a change in one variable causes a change in another variable, which is a more powerful property.</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<h3 style = \"text-align:center\" > 5. Optional bonus</h3>  \n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    "> To give you an idea of how you can use your work so far, here are two examples of how it can be used.\n",
    ">\n",
    "* **(x)** **Visualization** \n",
    "> Plot the Monthly Average Production per Well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' to datetime if not already\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Create a monthly average by Well_ID\n",
    "monthly_avg = (\n",
    "    df.groupby(['Well_ID', pd.Grouper(key='Date', freq='M')])['Flow_bbl_day']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Enlarge the figure\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot each well\n",
    "for well in monthly_avg['Well_ID'].unique():\n",
    "    subset = monthly_avg[monthly_avg['Well_ID'] == well]\n",
    "    plt.plot(subset['Date'], subset['Flow_bbl_day'], label=well)\n",
    "\n",
    "# Beautify chart\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Flow (bbl/day)')\n",
    "plt.title('Monthly Average Production per Well')\n",
    "plt.legend(title=\"Well\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(y)** **Prepare for ML or predictive analysis**\n",
    "> Predict `Incident` based on features (`Flow_bbl_day`, `Pressure_bar`, `Temperature_C`, `Vibration_level`)  \n",
    "> and display the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = df[['Flow_bbl_day', 'Pressure_bar', 'Temperature_C', 'Vibration_level']]\n",
    "y = df['Incident'].map({'Yes':1, 'No':0})  # encode target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "owner": "DataScientest",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
