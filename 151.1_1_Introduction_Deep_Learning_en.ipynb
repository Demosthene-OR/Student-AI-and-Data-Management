{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Demosthene-OR/Student-AI-and-Data-Management/blob/main/151.1_1_Introduction_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "<img src=\"https://prof.totalenergies.com/wp-content/uploads/2024/09/TotalEnergies_TPA_picto_DegradeRouge_RVB-1024x1024.png\" height=\"150\" width=\"150\">\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<center><h1> Introduction to Deep Learning with Keras </h1></center>\n",
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    ">Since 2012, deep learning algorithms seem ready to solve many problems: recognizing faces as proposed by DeepFace, defeating poker players, enabling autonomous cars to drive, and even searching for cancer cells.\n",
    ">\n",
    ">\n",
    ">\n",
    ">However, the foundations of these methods are not so recent. Deep learning was formalized in 2007 based on new neural network architectures pioneered by McCulloch and Pitts in 1943. This was followed by numerous developments such as the perceptron (artificial neuron), the convolutional neural networks of Yann LeCun and Yoshua Bengio in 1998, and the deep neural networks that emerged in 2012. This work paved the way for numerous fields of application such as image processing (computer vision), language processing (NLP), and speech recognition.\n",
    ">\n",
    ">Furthermore, before the advent of deep learning, mathematical analysis methods already existed to address these issues. In computer vision, for example, Haar features and image gradients were used.\n",
    "<br>\n",
    "\n",
    "## Deep Learning vs. Machine Learning\n",
    "\n",
    "<br>\n",
    "\n",
    "> <img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/notebooks/masterclass_deeplearning_debutant_MLetDeep.png\" width=\"300\" height=\"200\">\n",
    ">\n",
    ">\n",
    ">\n",
    "> \n",
    "> **Deep learning** algorithms can be considered both a sophisticated and mathematically complex evolution of **machine learning** algorithms. The field has been the subject of much attention lately, and for two good reasons:\n",
    "> \n",
    ">\n",
    ">* Recent developments have led to results that were previously unthinkable. These new machine learning techniques take advantage of the massive increase in data, as well as phenomenal computing power thanks to graphics processors. Unlike other machine learning models, which reach a point of stagnation, a deep learning model will perform better the more data it has available. \n",
    "\n",
    "> <img src= \"https://datascientest.com/wp-content/uploads/2020/06/DL5.png\" >\n",
    "\n",
    ">* Deep learning describes algorithms that analyze data with a logical structure similar to the way a human would draw conclusions. Note that this can occur through both supervised and unsupervised learning. To achieve this, deep learning applications use a layered structure of algorithms called an **artificial neural network** (ANN). The design of such an ANN is inspired by the biological neural network of the human brain, leading to a much more efficient learning process than standard machine learning models.\n",
    ">\n",
    "\n",
    "\n",
    "\n",
    "## **What are the differences?**\n",
    "\n",
    ">First, while traditional machine learning algorithms have a fairly simple structure, such as linear regression or a decision tree, deep learning is based on artificial neural networks, like the human brain, which are complex and intertwined.\n",
    ">\n",
    ">\n",
    ">Second, deep learning algorithms require much less human intervention. In machine learning, a data scientist must specify the model parameters, train the model, observe the results, and then readjust the model. In deep learning, **learning takes place continuously** through a number of iterations defined in advance.\n",
    ">\n",
    ">This continuous learning through a complex network makes it possible to more effectively handle problems where there are **more parameters than observations**, and for which explicit mathematical solutions often do not exist.\n",
    ">\n",
    "> This video illustrates and complements the differences outlined above (click on the image to be redirected to the video):\n",
    ">\n",
    ">\n",
    "> [![This video illustrates and expands on the differences outlined above:](https://img.youtube.com/vi/q6kJ71tEYqM/0.jpg)](https://www.youtube.com/watch?v=q6kJ71tEYqM \"Machine Learning vs. Deep Learning: What are the Differences? \")\n",
    "<!-- [![This video illustrates and expands on the differences outlined above:](https://img.youtube.com/vi/DazUaVu5MO0/0.jpg)](https://www.youtube.com/DazUaVu5MO0 \"Machine Learning vs. Deep Learning: What are the Differences? \") -->\n",
    "\n",
    "## Some examples of applications\n",
    "\n",
    "\n",
    "As mentioned above, deep learning has many potential applications, from image processing to language processing and speech recognition.\n",
    "\n",
    "> * The interpreting profession is being disrupted by the advent of audio and handwriting recognition and text translation models: \n",
    ">\n",
    ">\n",
    "><img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/notebooks/masterclass_deeplearning_debutant_translate.jpeg\" width=\"400\" height=\"200\"> \n",
    "\n",
    "\n",
    "> * Autonomous cars:\n",
    ">\n",
    "><img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/notebooks/masterclass_deeplearning_debutant_intro_cars.gif\" alt=‘animated’ width=\"400\" height=\"200\">\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<h2 style = “text-align:center”>How Deep Learning works </h2> \n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## What is a perceptron?\n",
    "\n",
    "\n",
    "> Deep learning algorithms are based on neural networks, which are **layered architectures**: one layer takes our data as input, hidden layers process this data, and an output layer returns the expected result based on our problem. The GIF below illustrates how they work: \n",
    ">\n",
    "><img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/notebooks/masterclass_deeplearning_debutant_intro_dense.gif\" style=‘width:400px’>\n",
    ">\n",
    "> Each layer is composed of one or more neurons: **the perceptron**. It works as follows: it takes a vector of parameters as input, undergoes several transformations before being interpretable by the algorithm, and returns an output value. It generally contains:\n",
    ">\n",
    ">\n",
    ">* A weight vector $w$\n",
    ">\n",
    ">* A bias $b$ \n",
    ">\n",
    ">* An activation function $f$\n",
    ">\n",
    ">The scalar product between the weight vector $w$ and a vector $x$ is denoted as:\n",
    ">\n",
    ">$$w^\\top x := w_1 x_1 + ... + w_n x_n$$\n",
    ">\n",
    ">\n",
    ">For an input vector $x$, the output of the model's perceptron $h$ is:\n",
    ">\n",
    ">$$ h = \\mathrm{Perceptron}(x) = f(w^\\top x + b)$$\n",
    "> \n",
    ">Here is an illustration: \n",
    "><img src = \"https://assets-datascientest.s3-eu-west-1.amazonaws.com/notebooks/perceptron1.png\" width=\"900\" height=\"200\">\n",
    "\n",
    "> It is important to note that if the activation function chosen is the identity function, the perceptron only calculates a linear operation. In this specific case, the task performed by the neuron is nothing more than **linear regression**. This is useful when the data is linearly separable, as you can see in several geometric representations below.\n",
    "\n",
    "## The scalar product\n",
    "\n",
    "\n",
    ">As seen previously, the scalar product is the basis of how a perceptron works.\n",
    ">\n",
    "><br>**What is its purpose?** \n",
    ">\n",
    "> It is used for classification thanks to its geometric properties. The result produced by the scalar product between two vectors is very easy to interpret.\n",
    ">\n",
    "> In the following interactive figure, you can see a point and a vector. The blue point **x** with coordinates **${(x_1, x_2)}$** is the point we need to classify. The green vector **w** with coordinates **${(w_1, w_2)}$** is the vector that will allow us to classify it.\n",
    ">\n",
    "> Classification with the scalar product is done as follows:\n",
    ">\n",
    ">\n",
    "> * If the scalar product between **x** and **w** is **positive**, **x will be classified as 1** (blue).\n",
    ">\n",
    ">\n",
    "> * If the scalar product between **x** and **w** is **negative**, **x will be classified as 0** (red).\n",
    ">\n",
    ">\n",
    "> The **green line perpendicular to w** represents all points in the domain such that **the scalar product with w is 0**. This line is called the **decision boundary** of the classification problem.\n",
    "\n",
    "* **(a)** Run the following cell to display the interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5157bc62b45f48d2adfc87c67f9f2185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(axes=[Axis(scale=LinearScale(max=5.0, min=-5.0)), Axis(orientation='vertical', scale=LinearScale(max=5.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7572272c1142e7801287a1d2f195c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-2.0, description='x1', max=4.0, min=-4.0), FloatSlider(value=1.0, des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from interaction_dense import show_dotProduct\n",
    "show_dotProduct()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Linear Separability\n",
    "\n",
    "> The concept of linear separability of a database is fundamental to the use of scalar product classification. \n",
    ">\n",
    "> The following figure corresponds to the **Iris** database, which contains two variables: `Sepal Width` and `Sepal Length`, corresponding to the width and length of the sepals of two different species of iris. The classification problem is as follows:\n",
    ">\n",
    "#### Can we determine the species of a flower based on these two variables?\n",
    ">\n",
    ">\n",
    "> The **green points** in the figure correspond to flowers of the species **iris setosa**, and the **orange points** correspond to flowers of the species **iris virginica**. (The database has been normalized for better visualization).\n",
    "\n",
    "> To solve this problem **geometrically** with scalar product classification, we can reformulate the question as follows:\n",
    "\n",
    "* **(c)** **Is there a linear decision boundary that would allow us to separate the two species?**\n",
    "\n",
    "* **(d)** Using the following interactive figure, find a vector w that defines a decision boundary **separating the green points from the orange points**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b9a03abdf6402b96ecf1259f0d153f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(axes=[Axis(label='Sepal Length', scale=LinearScale(max=4.0, min=-4.0)), Axis(label='Sepal Width', orien…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257f911d517445b9854d60dc0b682a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='w1', max=4.0, min=-4.0, step=0.11), FloatSlider(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from interaction_dense import show_data\n",
    "show_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "> One possible solution is the vector `w = (-1.8, 0.95)`, which defines a **linear** decision boundary that **perfectly** separates the two groups of individuals. We then say that the database is **linearly separable**.\n",
    ">\n",
    "> In this particular case, the decision boundary is called a **separating hyperplane**. We use this name because the points defining the decision boundary are the points satisfying the **plane equation** ${\\{ x = (x_1,x_2)\\in \\mathbb{R}^2 : \\langle x, w \\rangle = x_1 w_1 + x_2 w_2 = 0\\} }$\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "> We have seen that we can find a hyperplane for the Iris database that perfectly separates the two species of flowers. However, this solution was found visually.\n",
    ">\n",
    "><br> **How can we find a separating hyperplane mathematically?**\n",
    ">\n",
    "> First, we need to find a way to quantify the quality of the separation of the species. One of the simplest possibilities is to **count the number of classification errors we would make using a specific vector**.\n",
    ">\n",
    "> Suppose that our database contains $n$ points ${X = (x_i)_{i = 1, 2,..., n} \\in \\mathbb{R}^d}$ and that each of these points is associated with values ${Y = (y_i)_{i = 1,2,...,n} \\in \\{0,1\\}}$ corresponding to the group to which the point $x_i$ belongs.\n",
    ">\n",
    "> In our example, class 1 would correspond to the species *iris setosa* and class 0 to the species *iris virginica*.\n",
    ">\n",
    "> Mathematically, the classification of an individual $x_i$ by a vector $w$ would be done by a function $f$ defined as follows:\n",
    ">\n",
    ">$${f(x_i, w) = \\mathbb{1}_{\\langle x,w \\rangle \\geq 0} = \\begin{cases} 1 & \\mbox{if } \\langle x,w \\rangle \\geq 0 \\\\  0 & \\mbox{if } \\langle x,w \\rangle < 0 \\end{cases}}$$\n",
    ">\n",
    "> Thus, the number of errors can be calculated by a function of ${w = (w_1, w_2)}$, $X$ and $Y$ that would be written as follows:\n",
    ">\n",
    "> $${ g(w,X,Y) = \\sum_{i = 1}^{n}\\mathbb{1}_{f(x_i, w)\\neq y_i} }$$\n",
    ">\n",
    "> This function allows us to define a criterion for determining the best solution to our classification problem. Functions of this type are called **loss functions**. \n",
    ">\n",
    "> The lower the value of this function, the more effective our classification function is. **Minimizing this loss function with respect to vector w is therefore equivalent to finding a separating hyperplane**.\n",
    ">\n",
    "> There are different loss functions depending on the problem to be solved. Here are a few examples (which you don't need to remember for now) with their names in the Tensorflow/Keras library that you will be using in this module: \n",
    ">\n",
    "| TensorFlow function     | Application | Meaning |\n",
    "| :-------------: |:-------------:|:-------------:|\n",
    "| `BinaryCrossentropy`   | Binary classification | Cross entropy in the case of a binary classification problem |\n",
    "| `CategoricalCrossentropy`   | Multiple classification | Cross entropy with y_true in the form of a one-hot vector |\n",
    "| `SparseCategoricalCrossentropy` | Multiple classification | Cross entropy with y_true in the form of a class index |\n",
    "| `Hinge` | Binary/multiple classification | Loss function that only takes into account the error of points in the margin (zero loss function for points that are too easy to predict) | \n",
    "| `mse`   | Regression problem | Mean squared error | \n",
    "| `mae`   | Regression problem | Mean absolute error | \n",
    "| `mape`   | Regression problem | Mean absolute percentage error|\n",
    "\n",
    "* Run the following cell to display some loss functions for our Iris classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29dda26c08d43469fbbc9ff7349d5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Figure(axes=[Axis(label='Sepal Length', scale=LinearScale(max=4.0, min=-4.0)), A…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dl_widgets import show_losses\n",
    "show_losses()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Review of the Perceptron algorithm and activation function\n",
    "> A  special feature of the Perceptron algorithm is that we will use a function called **activation**, which will allow us to modify our perceptron output data to use a loss function that is more suited to our problem.\n",
    ">\n",
    ">Among the most commonly used **activation functions** are:\n",
    ">- *Sigmoid*\n",
    ">- *Tanh*\n",
    ">- *ReLU (Rectified Linear Unit)*\n",
    ">- *Leaky ReLU (Rectified Linear Unit)*\n",
    "> \n",
    "> Their graphical representation can be found in the figure below:\n",
    ">\n",
    "><img src= https://assets-datascientest.s3-eu-west-1.amazonaws.com/notebooks/masterclass_deeplearning_activation_functions.png style=\"width:800px\">\n",
    "> <p style=\"text-align:center\"> <i>Graphical representation of the main activation functions, <a href=\"https://medium.com/@shrutijadon10104776/survey-on-activation-functions-for-deep-learning-9689331ba092\">source</a></i></p>\n",
    ">\n",
    ">\n",
    "> Let's assume that $(y_i)_{i = 1,..,n} \\in \\{-1, 1\\} $ and that we use the *tanh* function as activation. The classification function becomes:\n",
    ">\n",
    "> $$ f(x_i, w) = tanh(\\langle x, w \\rangle + bias) $$\n",
    ">\n",
    "> The loss function then becomes the sum of the comparisons between our perceptron output values after the activation function and the expected labels (-1 or 1):\n",
    "> $$ g(w, X, Y) = \\sum_{i = 1}^{n} (f(x_i) - y_i)^2 $$\n",
    ">\n",
    "> The classification function and the loss function **no longer contain indicator functions** and are now **differentiable at every point**. This distinction is very important for what follows. Furthermore, the loss function in our example is **convex** and has a single minimum. (This is true if the data is linearly separable, which is almost never the case in practice).\n",
    ">\n",
    "> Another reason we would use the *tanh* function is that\n",
    ">\n",
    "> $$ tanh(\\langle x, w \\rangle + bias) = 0 \\iff \\langle x, w \\rangle + bias = 0$$\n",
    "> because $$ tanh(x) = 0 \\iff x = 0 $$\n",
    ">\n",
    "> That is, the equation of the optimal separating hyperplane is the same as if we were not using an activation function.\n",
    "\n",
    "## Gradient Descent Training\n",
    "\n",
    "> Thanks to the activation function, the loss function is differentiable. We can use a **gradient descent algorithm** to find the optimal vector $w$.\n",
    ">\n",
    "> The gradient descent algorithm is very simple. The easiest case to illustrate is the one-dimensional case. In the following figure, the function represented is $f(x) = x^2$ and its derivative is $f'(x) = 2x$.\n",
    "\n",
    "* **(e)** Run the following cell to display the interaction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18381cfbd9f343d281a4f42da3cec172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(axes=[Axis(scale=LinearScale(max=10.0, min=-10.0)), Axis(orientation='vertical', scale=LinearScale(max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ddcee689be34c9daa39de4b3581aaf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-5.0, description='x', max=10.0, min=-10.0, step=0.2), Output()), _dom…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from interaction_dense import show_optimization_square\n",
    "show_optimization_square()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "> When $f'(x) < 0$ (in red), then $f$ is **decreasing** in the neighborhood of $x$.\n",
    ">\n",
    "> When $f'(x) > 0$ (in green), then $f$ is **increasing** in the neighborhood of $x$.\n",
    ">\n",
    "> Thus, a point $x_{min}$ is a minimum of a **convex** function if and only if $f'(x_{min}) = 0$, i.e., $f$ must be increasing in the neighborhood of any point $x > x_{min}$ and decreasing in the neighborhood of any point $x < x_{min}$\n",
    ">\n",
    "> Let $x_0$ be a random point in the domain of $f$. The gradient descent algorithm then consists of choosing a point in the **opposite** direction to the gradient. That is:\n",
    ">\n",
    "> * If $f'(x_0) < 0$, $f$ is decreasing in the neighborhood of $x_0$, which means that the minimum $x_{min}$ is necessarily greater than $x_0$.\n",
    ">\n",
    ">\n",
    "> * If $f'(x_0) > 0$, $f$ is increasing in the neighborhood of $x_0$, which means that the minimum $x_{min}$ is necessarily less than $x_0$.\n",
    ">\n",
    "> We then define $x_1 = x_0 - \\lambda f'(x_0)$, where $\\lambda$ is called **the descent step**. In the context of the Perceptron algorithm and deep learning in general, $\\lambda$ is called the **learning rate**.\n",
    ">\n",
    ">\n",
    "> We repeat the operation until we obtain a point $x_k$ such that $|f'(x_k)| < tol$, where $tol$ is the **tolerance**, a very small constant.\n",
    ">\n",
    "> > Step 0: Define an initial point $x_0$ and a tolerance $tol$.\n",
    "> >\n",
    "> > Step k: As long as $|f'(x_k)| >= tol$: $x_{k+1} = x_k - \\lambda f'(x_k)$.\n",
    "\n",
    "* **(f)** What happens when the descent step is too small? When it is too large?\n",
    "\n",
    "\n",
    "* **(g)** For the initialization $x_0 = -10$, find the smallest descent step such that $|f'(x_k)| \\leq 0.001$ after 20 steps.\n",
    "\n",
    "\n",
    "* **(h)** Find a descent step such that the descent algorithm converges to the minimum in 1 step for any initialization $x_0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6971df3b5aa4032bdf0eb8bf6af9d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(axes=[Axis(scale=LinearScale(max=10.0, min=-10.0)), Axis(orientation='vertical', scale=L…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from interaction_dense import show_gradient_descent\n",
    "show_gradient_descent()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Limitations of gradient descent\n",
    "\n",
    "> As you can see, the gradient descent algorithm with the right descent step size is very effective at finding the global minimum of a function. However, this algorithm has one major weakness: **it is only effective when the function to be minimized is strictly convex, which is not always the case (as we will see below)**.\n",
    ">\n",
    "> In the following interactive figure, we have plotted the function $f(x) = (\\frac{x}{5})^4 + (\\frac{x}{5})^3 - 6(\\frac{x}{5})^2 + 1$.\n",
    ">\n",
    "> This function contains a global minimum (the one we want to approach) and a local minimum (the one we want to avoid).\n",
    "\n",
    "* **(i)** What happens if we apply the gradient descent algorithm with initialization $x_0 = 11$ and a descent step of $0.1$?\n",
    "\n",
    "\n",
    "* **(j)** What happens if we apply the gradient descent algorithm with initialization $x_0 = 0$ and any descent step?\n",
    "\n",
    "\n",
    "* **(k)** What happens if we apply the gradient descent algorithm with initialization $x_0 = -1$ and a descent step of $0.1$?\n",
    "\n",
    "\n",
    "* **(l)** What happens if we apply the gradient descent algorithm with initialization $x_0 = -1$ and a descent step greater than $0.54$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05e197da1bb4f39889a2fc266db30b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(axes=[Axis(scale=LinearScale(max=15.0, min=-16.0)), Axis(orientation='vertical', scale=L…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from interaction_dense import show_optimization\n",
    "show_optimization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "> When the function to be minimized is not convex, the gradient descent algorithm becomes unpredictable and does not produce consistent results. **The results of the algorithm are very sensitive to variations in the gradient step size**.\n",
    ">\n",
    "> In the vast majority of cases in deep learning, **the loss function to be minimized is never convex, and the gradient descent algorithm will converge to a local minimum**.\n",
    ">\n",
    "> Unfortunately, the gradient descent algorithm is one of the only algorithms that can be used in practice because it is the only effective optimization algorithm given our computing capabilities.\n",
    ">\n",
    ">\n",
    "> As you will see later in the practical modules, **the gradient step size is one of the most influential hyperparameters on the performance of a deep learning model***.\n",
    ">\n",
    "> In the next notebooks, you will see how we can use these loss and activation functions to solve problems using neural networks. We will also use objects that you are already familiar with and that allow us to evaluate models: **metrics**. You may be wondering why we don't just use metrics to optimize our perceptrons, in this case **accuracy** for example, since we are dealing with a classification problem. Take a look at the following widget, paying particular attention to the differentiability and convexity of the loss function with respect to the metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d055fc42c3c14c52bc9ea04f0761b309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Figure(axes=[Axis(label='Sepal Length', scale=LinearScale(max=4.0, min=-4.0)), A…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dl_widgets import show_accuracy\n",
    "show_accuracy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<center><h1> Key points to remember </h1></center>\n",
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    "\n",
    "> * The scalar product is the main tool we use for classification. **This classification is purely geometric**.\n",
    "> * The objective of a Perceptron is to **find a hyperplane that separates the different classes of individuals**. \n",
    "> * This objective is achieved by **minimizing the loss function using gradient descent**.\n",
    "> * There are a number of **activation functions** that address different issues, and their importance will be even greater for multilayer Perceptrons.\n",
    "> * If the database is not linearly separable, there is not necessarily a single global minimum.\n",
    "> * **A gradient step that is too large or too small will prevent the MLP algorithm from converging to a satisfactory solution**. Finding the right gradient step and initialization is the challenge of deep learning.\n",
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<center><h1> Going further - The Perceptron algorithm </h1></center>\n",
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    "\n",
    "> In this exercise, we will introduce the operating principle of the perceptron algorithm. To do this, we will train a model based on the simple perceptron principle using the *Moon* database from the *scikit-learn* library.\n",
    ">\n",
    "> As mentioned earlier, the **choice** of activation function depends on **the desired output space** of the Perceptron model:\n",
    ">\n",
    "> * The $\\mathbf{sigmoid}$ function takes values from $(-\\infty, \\infty) $ to $ [0,1]$, making it an ideal choice for displaying a probability value for classification. In this case, the Perceptron model is equivalent to **logistic regression**.\n",
    ">\n",
    "> Here is the link to the exercise on [colab](https://colab.research.google.com/drive/1QzbcVRvXCTjgtDoF6EKzYX_UM6OyglNt?usp=sharing) **choice**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "owner": "DataScientest"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
