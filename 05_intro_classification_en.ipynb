{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Demosthene-OR/Student-AI-and-Data-Management/blob/main/05_intro_classification_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "<img src=\"https://prof.totalenergies.com/wp-content/uploads/2024/09/TotalEnergies_TPA_picto_DegradeRouge_RVB-1024x1024.png\" height=\"150\" width=\"150\">\n",
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<center><H1> Introduction to Machine Learning with Scikit-learn </H1></center> \n",
    "<center><H2> Part II: Simple Classification Models </H2></center>\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    "> In this second part of the introduction to the `scikit-learn` module, we will focus on the second type of problem in Machine Learning: the problem of **classification**.\n",
    "> \n",
    "> The objective of this introduction is:\n",
    ">> * To introduce the problem of classification.\n",
    ">>\n",
    ">>\n",
    ">> * To learn how to use the `scikit-learn` module to build a classification model, also known as a “classifier.”\n",
    ">>\n",
    ">>\n",
    ">> * To introduce useful metrics for evaluating model performance.\n",
    "\n",
    "## Introduction to classification\n",
    "\n",
    "### Purpose of classification\n",
    "\n",
    "> In supervised learning, the goal is to predict the value of a target variable based on explanatory variables.\n",
    ">> * In a **regression** problem, the target variable takes **continuous values**. These values are numerical: the price of a house, the amount of oxygen in the air in a city, etc. <br> The target variable can therefore take an **infinite number of values**.\n",
    ">>\n",
    ">>\n",
    ">> * In a **classification** problem, the target variable takes **discrete values**. These values can be numerical or literal, but in both cases, the target variable takes a **finite number of values**. <br>\n",
    "> The different values taken by the target variable are called **classes**. \n",
    ">\n",
    "> **The objective of classification is therefore to predict the class of an observation based on its explanatory variables.**\n",
    "\n",
    "### An example of classification\n",
    "\n",
    "> Let's take an example of **binary** classification, in other words where there are **two** classes. <br>\n",
    "> We want to determine whether the water in a stream is drinkable or not based on its concentration of toxic substances and its mineral salt content. \n",
    ">\n",
    "> The two classes are therefore **‘drinkable’** and **‘undrinkable’**. \n",
    ">\n",
    "> <br>\n",
    "> <img src = 'https://assets-datascientest.s3-eu-west-1.amazonaws.com/train/sklearn_intro_classification_binaire.png' style = \"height:400px\">\n",
    "> <br>\n",
    ">\n",
    "> In the figure above, each point represents a stream whose position on the map is defined by its toxic substance concentration and mineral salt content values. \n",
    "> \n",
    "> The objective will be to build a **model capable of assigning one of the two classes** (‘drinkable’/'undrinkable') to a stream for which only these two variables are known.\n",
    ">\n",
    "> The figure above suggests the existence of two zones that make it easy to classify streams:\n",
    ">> * A zone where streams are drinkable (top left).\n",
    ">>\n",
    ">>\n",
    ">> * An area where streams are non-potable (bottom right).\n",
    ">\n",
    "> We would like to create a model capable of **separating the dataset into two parts** corresponding to these areas. \n",
    ">\n",
    "> A simple technique would be to separate the two areas **using a line**.\n",
    "\n",
    "* **(a)** Run the following cell to display the interactive figure.\n",
    "> * The **orange** dots are **drinkable** streams and the **blue** dots are **undrinkable** streams.\n",
    "> \n",
    "> * The **red arrow** corresponds to a **vector** defined by $w = (w_1, w_2)$. The red line corresponds to the plane orthogonal (i.e., perpendicular) to $w$. You can modify the coordinates of the vector $w$ in two ways:\n",
    ">> * By scrolling the sliders `w_1` and `w_2`.\n",
    ">>\n",
    ">>\n",
    ">> * By clicking on the values to the right of the sliders and then directly entering the desired value.\n",
    "\n",
    "\n",
    "* **(b)** Try to find a vector $w$ such that **the plane orthogonal to $w$ perfectly separates the two classes of streams**.\n",
    "\n",
    "\n",
    "* **(c)** One possible solution is given by the vector $w = (-1.47, 0.84)$. Does the vector $w = (1.47, -0.84)$ also give a solution?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/Demosthene-OR/Student-AI-and-Data-Management/main/classification_widgets.py\n",
    "from classification_widgets import linear_classification\n",
    "\n",
    "linear_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "> The classification we have just performed is **linear**, meaning that we used a linear plane to separate our classes.\n",
    ">\n",
    "> Thus, the objective of linear classification models is to find the vector $w$ that best separates the different classes. <br>\n",
    "> Each linear model has its own technique for finding this vector.\n",
    ">\n",
    "> There are also non-linear classification models, which we will look at later.\n",
    ">\n",
    "> <br>\n",
    "> <img src = 'https://assets-datascientest.s3-eu-west-1.amazonaws.com/train/sklearn_intro_classification_lin_non_lin.png' style = “height:400px”>\n",
    "\n",
    "## 1. Using `scikit-learn` for classification\n",
    "\n",
    "> We will now introduce the main tools of the `scikit-learn` module that are essential for solving a classification problem.\n",
    ">\n",
    "> In this exercise, we will use the [Congressional Voting Records](https://archive.ics.uci.edu/ml/datasets/congressional+voting+records) dataset, which contains a number of votes cast by members of the U.S. House of Representatives.\n",
    ">\n",
    "> The objective of our classification problem will be to **predict the political party** (“Democrat” or “Republican”) of members of the House of Representatives based on their votes on issues such as education, health, the budget, etc. \n",
    ">\n",
    "> The explanatory variables will therefore be votes on different issues, and the target variable will be the political party “Democrat” or “Republican.”\n",
    ">\n",
    "> To solve this problem, we will use a linear classification model: **Logistic Regression**.\n",
    "\n",
    "\n",
    "### Data preparation\n",
    "\n",
    "* **(a)** Run the following cell to import the `pandas` and `numpy` modules needed for the rest of the exercise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "* **(b)** Load the data contained in the `‘votes.csv’` file into a `DataFrame` named `votes`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/Demosthene-OR/Student-AI-and-Data-Management/main/data/\"\n",
    "votes = pd.read_csv(url+'votes.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "To briefly visualize our data:\n",
    "\n",
    "* **(c)** Display the number of rows and columns in `votes`.\n",
    "\n",
    "\n",
    "* **(d)** Display a preview of the first 20 rows of `votes`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DataFrame dimensions\n",
    "print('The DataFrame has', votes.shape[0], 'rows and', votes.shape[1], 'columns.')\n",
    "\n",
    "# Displaying the first 20 rows\n",
    "votes.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "> * The first column, **“party,”** contains the name of the **political party** to which each member of the House of Representatives belongs.  \n",
    ">\n",
    ">\n",
    "> * The following **16 columns** contain each member of Congress's votes on proposed legislation: \n",
    ">> * `‘y’` indicates that the elected official voted **for** the proposed legislation.\n",
    ">>\n",
    ">>\n",
    ">> * `‘n’` indicates that the elected official voted **against** the proposed legislation.\n",
    ">\n",
    "> In order to use the data in a classification model, these columns must be converted into binary **numeric** values, i.e., either 0 or 1.\n",
    "\n",
    "* **(e)** For each of columns 1 to 16 (column 0 being our target variable), replace the values `‘y’` with 1 and `‘n’` with 0. To do this, we can use the **`replace`** method of the `DataFrame` class.\n",
    "\n",
    "\n",
    "* **(f)** Display the first 10 rows of the modified `DataFrame`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replacement of values\n",
    "votes = votes.replace(('y', 'n'), (1, 0))\n",
    "\n",
    "# Dataframe display\n",
    "votes.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "* **(g)** In a `DataFrame` named `X`, store the **explanatory** variables of the dataset (all columns except `‘party’`). To do this, you can use the **`drop`** method of a `DataFrame`.\n",
    "\n",
    "\n",
    "* **(h)** In a `Series` named `y`, store the **target variable** (`‘party’`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data separation\n",
    "\n",
    "X = votes.drop(['party'], axis = 1)\n",
    "y = votes['party']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "> As with regression, we will need to split the dataset into two parts: a **training** set and a **test** set. As a reminder:\n",
    ">> * The training set is used to **train the classification model**, i.e., to find the model parameters that best separate the classes.\n",
    ">>\n",
    ">>\n",
    ">> * The test set is used to **evaluate** the model on data it has never seen before. This evaluation will allow us to judge the model's ability to **generalize**.\n",
    "\n",
    "* **(i)** Import the `train_test_split` function from the `sklearn.model_selection` submodule. Remember that this function is used as follows:\n",
    ">python\n",
    ">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    ">\n",
    "\n",
    "\n",
    "* **(j)** Separate the data into a training set `(X_train, y_train)` and a test set `(X_test, y_test)`, keeping 20% of the data for the test sample.\n",
    "> To eliminate the randomness of the `train_test_split` function, you can use the `random_state` parameter with an integer value (e.g., `random_state = 2`). <br>\n",
    "> This way, every time you use the function with the argument `random_state = 2`, the resulting datasets will be the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "> The logistic regression model is closely related to the **linear regression** model seen in the previous notebook. \n",
    ">\n",
    "> They should not be **confused**, as they do not solve the same types of problems:\n",
    ">> * **Logistic** regression is used for classification (predicting classes).\n",
    ">>\n",
    ">>\n",
    ">> * **Linear** regression is used for regression (predicting a quantitative variable).\n",
    ">\n",
    "> The linear regression model was defined by the following formula:\n",
    "> $$ y \\approx \\beta_0 + \\sum_{j=1}^p \\beta_j x_j $$\n",
    ">\n",
    "> Logistic regression no longer estimates $y$ directly, but rather the **probability** that $y$ is equal to 0 or 1. <br>\n",
    "> Thus, the model is defined by the formula:\n",
    "> $$P(y = 1) = f(\\beta_0 + \\sum_{j=1}^p \\beta_j x_j)$$\n",
    ">\n",
    "> Where $$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
    ">\n",
    "> The function $f$, often called **sigmoid** or **logistic function**, transforms the linear combination $\\beta_0 + \\sum_{j=1}^p \\beta_j x_j$ into a value between 0 and 1 that can be interpreted as a **probability**:\n",
    ">> * If $\\beta_0 + \\sum_{j=1}^p \\beta_j x_j$ is positive, then $P(y = 1) \\gt 0.5$, so the predicted class of the observation will be 1.\n",
    ">>\n",
    ">>\n",
    ">> * If $\\beta_0 + \\sum_{j=1}^p \\beta_j x_j$ is negative, then $P(y = 1) \\lt 0.5$, i.e., $P(y = 0) \\gt 0.5$, so the predicted class of the observation will be 0.\n",
    "\n",
    "* **(k)** Import the `LogisticRegression` class from the `linear_model` submodule of `scikit-learn`.\n",
    "\n",
    "\n",
    "* **(l)** Instantiate a `LogisticRegression` model named **`logreg`** without specifying any constructor arguments.\n",
    "\n",
    "\n",
    "* **(m)** Train the model on the training dataset using the `fit` method of the `LogisticRegression` class.\n",
    "\n",
    "\n",
    "* **(n)** Make a prediction on the **test** data. Store these predictions in **`y_pred_test_logreg`** and display the first 10 predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the LogisticRegression class from sklearn's linear_model submodule\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate the model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "#  Train the model on the training set\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "y_pred_test_logreg = logreg.predict(X_test)\n",
    "\n",
    "# Displaying the first 10 predictions\n",
    "print(y_pred_test_logreg[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "## 2. Evaluating the performance of a classification model\n",
    "\n",
    "> There are various metrics for evaluating the performance of classification models, such as:\n",
    ">> * **Accuracy**.\n",
    ">> \n",
    ">>\n",
    ">> * **Precision and recall**.\n",
    ">\n",
    "> \n",
    "> Each metric evaluates the performance of the model using a different approach.\n",
    ">\n",
    "> In order to explain these concepts, we will introduce four very important terms. \n",
    ">\n",
    "> **Arbitrarily**, we will choose the class **‘republican’ as the positive class** (1) and **‘democrat’ as the negative class** (0).\n",
    ">\n",
    "> Thus, we will call:\n",
    ">> * **True positive (TP)** an observation classified as **positive** (‘Republican’) by the model and which is actually **positive** (‘Republican’).\n",
    ">>\n",
    ">>\n",
    ">> * **False positive (FP)**: an observation classified as **positive** (‘Republican’) by the model but which was actually **negative** (‘Democrat’).\n",
    ">>\n",
    ">>\n",
    ">> * **True negative (TN)**: an observation classified as **negative** (‘democrat’) by the model and which is actually **negative** (‘democrat’).\n",
    ">>\n",
    ">>\n",
    ">> * **False negative (FN)**: an observation classified as **negative** (‘Democrat’) by the model but which was actually **positive** (‘Republican’).\n",
    ">\n",
    "> <br>\n",
    "> <img src = \"https://assets-datascientest.s3-eu-west-1.amazonaws.com/train/sklearn_intro_positif_negatif.png\" style = \"height:300px'\">\n",
    "> <br>\n",
    ">\n",
    "> **Accuracy** is the most commonly used metric for evaluating a model. <br>\n",
    "> It simply corresponds to the rate of **correct** predictions made by the model.\n",
    ">\n",
    "> We assume that we have $n$ observations. <br>\n",
    "> We denote the number of True Positives by $\\mathrm{TP}$ and the number of True Negatives by $\\mathrm{TN}$. <br>\n",
    "> Accuracy is then given by:\n",
    "> $$\\mathrm{accuracy} = \\frac{\\mathrm{TP} + \\mathrm{TN}}{n}$$\n",
    "> \n",
    "> **Precision** is a metric that answers the question: **Of all the model's positive predictions, how many are true positives?**\n",
    ">\n",
    "> If we denote the number of False Positives in the model by $\\mathrm{FP}$, then precision is given by:\n",
    "> $$\\mathrm{precision} = \\frac{\\mathrm{VP}}{\\mathrm{VP} + \\mathrm{FP}}$$\n",
    ">\n",
    "> A high precision score tells us that the model does not blindly classify all observations as positive.\n",
    "> \n",
    "> **Recall** is a metric that quantifies the proportion of truly positive observations that have been correctly classified as positive by the model.\n",
    ">\n",
    "> If we denote $\\mathrm{FN}$ as the number of false negatives, then recall is given by:\n",
    "> $$\\mathrm{recall} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}$$\n",
    ">\n",
    "> A high recall score tells us that the model is able to detect truly positive observations well.\n",
    ">\n",
    "> The **confusion matrix** counts the values of TP, TN, FP, and FN for a dataset, which allows us to calculate the three previous metrics:\n",
    ">\n",
    "> $$\n",
    "\\mathrm{Confusion Matrix} = \\begin{bmatrix}\n",
    "                                    \\mathrm{VN} & \\mathrm{FP} \\\\\n",
    "                                    \\mathrm{FN} & \\mathrm{VP}\n",
    "                                \\end{bmatrix}\n",
    " $$\n",
    ">\n",
    "> The **`confusion_matrix`** function in the `sklearn.metrics` submodule can be used to generate the confusion matrix from a model's **predictions**:\n",
    ">\n",
    "> ```python\n",
    "> confusion_matrix(y_true, y_pred)\n",
    ">\n",
    "> ```\n",
    ">\n",
    ">> * **`y_true`** contains the **true** values of y.\n",
    ">>\n",
    ">>\n",
    ">> * **`y_pred`** contains the values **predicted** by the model.\n",
    ">\n",
    "> The confusion matrix can also be displayed using the **`pd.crosstab`** function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* **(a)** Import the **`accuracy_score`**, **`precision_score`**, and **`recall_score`** functions from the `sklearn.metrics` submodule.\n",
    "\n",
    "\n",
    "* **(b)** Display the confusion matrix of the predictions of the **`logreg`** model using **`pd.crosstab`**.\n",
    "\n",
    "\n",
    "* **(c)** Calculate the accuracy, precision, and recall of the predictions of the **`logreg`** model. To use the `precision_score` and `recall_score` metrics, you will need to specify the argument **`pos_label = ‘republican’`** to indicate that the `‘republican’` class is the positive class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "function": "solution",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Calculate and display the confusion matrix\n",
    "print(pd.crosstab(y_test, y_pred_test_logreg, rownames=['Reality'], colnames=['Prediction']))\n",
    "\n",
    "# Calculate accuracy, precision, and recall\n",
    "print(\"\\nLogReg Accuracy:\", accuracy_score(y_test, y_pred_test_logreg))\n",
    "\n",
    "print(\"\\nLogReg Precision:\", precision_score(y_test, y_pred_test_logreg, pos_label = 'republican'))\n",
    "\n",
    "print(\"\\nLogReg Recall:\", recall_score(y_test, y_pred_test_logreg, pos_label = 'republican'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Recap\n",
    "\n",
    "> Scikit-learn offers numerous classification models such as **`LogisticRegression`**.\n",
    ">\n",
    "> These models are used in the same way as **all** scikit-learn models:\n",
    ">> * **Instantiation** of the model.\n",
    ">>\n",
    ">> \n",
    ">> * **Train** the model: **`model.fit(X_train, y_train)`**.\n",
    ">>\n",
    ">>\n",
    ">> * **Predict**: **`model.predict(X_test)`**.\n",
    ">\n",
    "> Prediction on the test set allows us to **evaluate** the model's performance using appropriate **metrics**.\n",
    ">\n",
    "> The metrics we have seen are used for **binary** classification and are calculated using four values:\n",
    ">> * True Positives:  Prediction = **+** | Reality = **+**\n",
    ">>\n",
    ">>\n",
    ">> * True Negatives: Prediction = **-** | Reality = **-**\n",
    ">>\n",
    ">>\n",
    ">> * False Positives:  Prediction = **+** | Reality = **-**\n",
    ">>\n",
    ">>\n",
    ">> * False Negatives:  Prediction = **-** | Reality = **+**\n",
    ">\n",
    "> All these values can be calculated using the **confusion matrix** generated by the **`confusion_matrix`** function of the `sklearn.metrics` submodule or by the **`pd.crosstab`** function.\n",
    "> \n",
    "> Using these values, we can calculate metrics such as:\n",
    ">> * **Accuracy**: The proportion of correctly classified observations.\n",
    ">>\n",
    ">>\n",
    ">> * **Precision**: The proportion of true positives among all positive predictions made by the model.\n",
    ">>\n",
    ">>\n",
    ">> * **Recall**: The proportion of truly positive observations that were correctly classified as positive by the model.\n",
    ">\n",
    "> All these metrics can be obtained using the **`classification_report`** function from the **`sklearn.metrics`** submodule.\n",
    ">\n",
    "# Conclusion and resources\n",
    "\n",
    "> This module has introduced the Python programming language and its main libraries, which are very useful in the suite (Numpy, Pandas, scikit-learn). The Pandas library allows you to obtain data in the form of easily manipulable dataframes.\n",
    ">\n",
    "> **If you want to learn more advanced methods that build on this module, you can move on to the “105 Data Quality” module.**\n",
    ">\n",
    "> **If you want to apply the methods presented to other dataframes, you can do so with the “Sandbox” module. This module consists of a blank notebook in which data is available and on which you can code freely.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "owner": "DataScientest"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
